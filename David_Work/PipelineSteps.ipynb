{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ac13bc-e932-4257-8bef-1d9f3184b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHATGPT\n",
    "Using **LangChain** is a great choice for managing LLM pipelines, including Retrieval-Augmented Generation (RAG) tasks. Here’s how you can integrate LangChain into your project to build a multimodal LLM for financial documents:\n",
    "\n",
    "### Steps to Implement RAG with LangChain\n",
    "\n",
    "1. **Data Ingestion & Preprocessing**:\n",
    "   - **Document Loading**: Use LangChain’s built-in document loaders to import your financial documents (PDFs, earnings calls, transcripts).\n",
    "     - `langchain.document_loaders` provides support for PDF, CSV, text, and other file types.\n",
    "     - For financial documents, you might use **PDFLoader** or **PyMuPDFLoader** from LangChain.\n",
    "     ```python\n",
    "     from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "     loader = PyMuPDFLoader(\"financial_document.pdf\")\n",
    "     documents = loader.load()\n",
    "     ```\n",
    "   \n",
    "   - **Text Preprocessing**: Clean and preprocess the text using libraries like `nltk` or `spaCy` to handle financial jargon, remove noise, and tokenize text.\n",
    "\n",
    "2. **Embedding Financial Documents**:\n",
    "   - Convert your documents into embeddings using a pretrained financial model like **FinBERT** or a model from Hugging Face.\n",
    "   - LangChain has built-in support for various embedding models. You can directly use `OpenAIEmbeddings`, `HuggingFaceEmbeddings`, or **SentenceTransformers** to convert financial documents into vector representations.\n",
    "     ```python\n",
    "     from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "     embeddings = HuggingFaceEmbeddings(model_name=\"bert-base-uncased\")\n",
    "     document_embeddings = embeddings.embed_documents([doc.page_content for doc in documents])\n",
    "     ```\n",
    "\n",
    "3. **Storing and Indexing**:\n",
    "   - Use **FAISS** or **Pinecone** as your vector store to index the embeddings of financial documents for retrieval.\n",
    "   - LangChain supports vector stores like FAISS, which integrates easily for building a RAG pipeline.\n",
    "     ```python\n",
    "     from langchain.vectorstores import FAISS\n",
    "\n",
    "     vector_store = FAISS.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725c20af-3f27-48c7-bd95-ecbaa0175908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\David\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\David\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\David\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\David\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "loader = PyMuPDFLoader(\"PTLO 2023 Q4 10K.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6a0793-3979-408f-924f-d98c1f0f1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove unwanted symbols and special characters (like checkboxes)\n",
    "    text = re.sub(r'[☐☒]', '', text)  # Remove checkbox symbols\n",
    "\n",
    "    # Remove newlines and excessive whitespace\n",
    "    text = re.sub(r'\\n+', ' ', text)  # Replace newlines with space\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "\n",
    "    # Trim leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c999d4b3-a513-4d4e-8d6b-63a4e114f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = [clean_text(doc.page_content) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69172a90-bca0-483b-9ccb-3cf0e62f884a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If securities are registered pursuant to Section 12(b) of the Act, indicate by check mark whether the financial statements of the registrant included in the filing reflect the correction of an error to previously issued financial statements. Yes No Indicate by check mark whether any of those error corrections are restatements that required a recovery analysis of incentive-based compensation received by any of the registrant’s executive officers during the relevant recovery period pursuant to § 240.10D-1(b). Yes No Indicate by check mark whether the registrant is a shell company (as defined in Rule 12b-2 of the Act). Yes No The aggregate market value of the common stock held by non-affiliates of the registrant on June 23, 2023, the last business day of the Registrant's most recently completed second fiscal quarter, based on the closing price of the registrant's Class A common stock as reported by The Nasdaq Stock Market on that date, was approximately $ 981,393,224 . This calculation does not reflect a determination that certain persons are affiliates of the registrant for any other purpose. As of February 20, 2024, there were 55,649,664 shares of the registrant's Class A common stock, par value $0.01 per share, issued and outstanding.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f6c6ce1-dd97-4449-8a23-8766e805ef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map POS tags to WordNet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'  # Adjective\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'  # Verb\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'  # Noun\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'  # Adverb\n",
    "    else:\n",
    "        return 'n'  # Default to noun\n",
    "        \n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # POS (Part of Speech) tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Apply lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in pos_tags]\n",
    "\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d29bb0a-01a2-4ee3-8dc6-2108ebbf9835",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = [preprocess_text(text) for text in cleaned_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80bc7c0e-6bd0-41cf-81f9-cb3522c5af1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'security register pursuant section b act indicate check mark whether financial statement registrant include filing reflect correction error previously issue financial statement yes indicate check mark whether error correction restatement require recovery analysis incentivebased compensation receive registrant ’ executive officer relevant recovery period pursuant § db yes indicate check mark whether registrant shell company define rule b act yes aggregate market value common stock hold nonaffiliates registrant june last business day registrant recently complete second fiscal quarter base closing price registrant class common stock report nasdaq stock market date approximately calculation reflect determination certain person affiliate registrant purpose february share registrant class common stock par value per share issue outstanding'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dea442b6-35ae-4379-939a-42d1627d2769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n",
      "C:\\Users\\David\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\David\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# For now, using bert-base-uncased to get a model running. However the chunks (which is currently the amount of text per page) is quite large\n",
    "# and may be needed to be split up before stored.\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"bert-base-uncased\")\n",
    "document_embeddings = embeddings.embed_documents(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19301e8b-ba5e-4d37-a2bc-72bd2272e44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ef590dd-dd3e-4f52-a9c4-62cccc4c13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook AI similarity search allows us to get the closest vectors to a query vector\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f39b1099-85a5-48e2-a94a-a45fd2f20c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1a9915df410>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c4efac-7e10-4993-a028-0237b71e8bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
