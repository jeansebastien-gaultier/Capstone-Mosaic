{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Dealing with the Financial Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jean-sebastiengaultier/Desktop/UChicago/Q3/Capstone/capstone/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing functions for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() # Lowercase the text\n",
    "    \n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text) # Remove punctuation\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words]) # Remove stopwords\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 350/350 [00:00<00:00, 560kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 3.37MB/s]\n",
      "Downloading: 100%|██████████| 455k/455k [00:00<00:00, 4.87MB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 335kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer for token-based chunking\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to chunk text based on tokens\n",
    "def chunk_text_by_tokens(text, max_tokens=512):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(tokens), max_tokens):\n",
    "        chunk = tokens[i:i+max_tokens]\n",
    "        chunks.append(tokenizer.decode(chunk, skip_special_tokens=True))\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
     ]
    }
   ],
   "source": [
    "# Initialize text and image embedding models\n",
    "text_embedder = SentenceTransformer('all-MiniLM-L6-v2')  # Text embeddings\n",
    "image_embedder = SentenceTransformer('clip-ViT-B-32')    # Image embeddings (using CLIP model)\n",
    "\n",
    "# Function to process and embed images\n",
    "def embed_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert(\"RGB\")\n",
    "    # Resize image as needed by the CLIP model\n",
    "    img = img.resize((224, 224))\n",
    "    return image_embedder.encode(img)\n",
    "\n",
    "# Function to extract and embed images from PDFs\n",
    "def extract_text_and_images_with_preprocessing(pdf_path):\n",
    "    text = \"\"\n",
    "    images = []\n",
    "    image_embeddings = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            # Extract text from page\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text  # Append to overall text\n",
    "            \n",
    "            # Extract images from the page (updated to handle image extraction reliably)\n",
    "            for img in page.images:\n",
    "                # Get the image from its bbox\n",
    "                try:\n",
    "                    image_object = page.within_bbox((img['x0'], img['top'], img['x1'], img['bottom'])).to_image()\n",
    "                    # Save the image temporarily to a bytes buffer and open with Pillow\n",
    "                    img_bytes = io.BytesIO()\n",
    "                    image_object.save(img_bytes, format='PNG')\n",
    "                    img_bytes.seek(0)\n",
    "                    pil_img = Image.open(img_bytes)\n",
    "\n",
    "                    # Resize and convert image as required by the CLIP model\n",
    "                    pil_img = pil_img.convert(\"RGB\")\n",
    "                    pil_img = pil_img.resize((224, 224))\n",
    "\n",
    "                    # Embed the image\n",
    "                    img_embedding = image_embedder.encode(pil_img)\n",
    "                    images.append(pil_img)  # Store the image\n",
    "                    image_embeddings.append(img_embedding)  # Store the embedding\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image on page {page_num}: {e}\")\n",
    "\n",
    "    # Preprocess and chunk the text\n",
    "    text = preprocess_text(text)\n",
    "    chunks = chunk_text_by_tokens(text, max_tokens=512)\n",
    "\n",
    "    return chunks, images, image_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image on page 0: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 2: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 3: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 4: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 8: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 8: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 8: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 8: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 12: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 14: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 16: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 17: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 19: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 20: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 21: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 22: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 23: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 25: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 27: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 29: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 31: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 33: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 35: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 36: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 37: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 38: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 39: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 40: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 40: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 41: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 42: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 43: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 44: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 45: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 46: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 47: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 48: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 49: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 50: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 51: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 53: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 54: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 55: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 56: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 57: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 58: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 59: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 61: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 62: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 65: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 66: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 69: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 70: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 71: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 72: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 73: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 74: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 75: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 76: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 77: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 78: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 79: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 80: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 81: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 82: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 83: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 84: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 85: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 86: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 87: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 88: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 89: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 90: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 91: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 92: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 93: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 94: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 95: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 96: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 97: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 98: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 99: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 100: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 101: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 102: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 103: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 104: type object 'PdfDocument' has no attribute '_process_page'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (47209 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image on page 0: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 0: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 0: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 0: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 0: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 0: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 0: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 0: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 0: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 0: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 0: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 2: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 15: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 15: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 16: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 17: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 18: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 19: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 20: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 21: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 22: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 23: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 24: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 25: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 26: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 27: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 28: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 29: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 30: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 31: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 32: type object 'PdfDocument' has no attribute '_process_page'\n",
      "Error processing image on page 33: type object 'PdfDocument' has no attribute '_process_page'\n"
     ]
    }
   ],
   "source": [
    "def process_pdfs_with_chunking_and_image_embedding(data_folder):\n",
    "    documents = []\n",
    "    for filename in os.listdir(data_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(data_folder, filename)\n",
    "            chunks, images, image_embeddings = extract_text_and_images_with_preprocessing(pdf_path)\n",
    "            documents.append({\n",
    "                'chunks': chunks,\n",
    "                'images': images,\n",
    "                'image_embeddings': image_embeddings,\n",
    "                'filename': filename\n",
    "            })\n",
    "    return documents\n",
    "\n",
    "# Folder containing your PDFs\n",
    "data_folder = '../Data/'\n",
    "\n",
    "# Process the PDFs with chunking and image embedding\n",
    "documents = process_pdfs_with_chunking_and_image_embedding(data_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a slight issue with the image embedding so far, but the text has been preprocessed and remoed any stopwords and puctuation as well as split into chunks of identical size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the chunks\n",
    "def embed_chunks(documents):\n",
    "    for document in documents:\n",
    "        document['chunk_embeddings'] = [{'text': chunk, 'embedding': text_embedder.encode(chunk, convert_to_tensor=True)} for chunk in document['chunks']]\n",
    "    return documents\n",
    "\n",
    "# Embed the chunks of all PDFs\n",
    "embedded_documents = embed_chunks(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "build_folder = 'Build/'\n",
    "if not os.path.exists(build_folder):\n",
    "    os.makedirs(build_folder)\n",
    "\n",
    "# Initialize FAISS index for text chunks and image embeddings\n",
    "embedding_dim_text = 384  # Dimension of the sentence transformer for text\n",
    "#embedding_dim_image = 512  # Dimension of CLIP model for image embeddings\n",
    "\n",
    "# Create separate FAISS indices for text and image embeddings\n",
    "index_text = faiss.IndexFlatL2(embedding_dim_text)\n",
    "#index_image = faiss.IndexFlatL2(embedding_dim_image)\n",
    "\n",
    "# Flatten all chunk embeddings and image embeddings and store their metadata\n",
    "all_chunk_embeddings = []\n",
    "#all_image_embeddings = []\n",
    "chunk_metadata = []\n",
    "#image_metadata = []\n",
    "\n",
    "for document in embedded_documents:\n",
    "    # Add text chunk embeddings to index\n",
    "    for chunk in document['chunk_embeddings']:\n",
    "        all_chunk_embeddings.append(chunk['embedding'].cpu().numpy())\n",
    "        chunk_metadata.append({'filename': document['filename'], 'text': chunk['text']})\n",
    "    \n",
    "    # Add image embeddings to index\n",
    "    # for img_embedding in document['image_embeddings']:\n",
    "    #     all_image_embeddings.append(img_embedding)\n",
    "    #     image_metadata.append({'filename': document['filename'], 'image': 'image path'})\n",
    "\n",
    "# Convert embeddings to numpy arrays\n",
    "all_chunk_embeddings = np.array(all_chunk_embeddings)\n",
    "#all_image_embeddings = np.array(all_image_embeddings)\n",
    "\n",
    "# Add embeddings to FAISS indices\n",
    "index_text.add(all_chunk_embeddings)\n",
    "#index_image.add(all_image_embeddings)\n",
    "\n",
    "# Save FAISS indices and metadata\n",
    "faiss.write_index(index_text, 'financial_docs_text_index.faiss')\n",
    "#faiss.write_index(index_image, 'Build/financial_docs_image_index.faiss')\n",
    "\n",
    "\n",
    "with open('financial_chunks_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(chunk_metadata, f)\n",
    "\n",
    "# with open('Build/financial_images_metadata.pkl', 'wb') as f:\n",
    "#     pickle.dump(image_metadata, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 0.99M/0.99M [00:00<00:00, 8.06MB/s]\n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 3.99MB/s]\n",
      "Downloading: 100%|██████████| 26.0/26.0 [00:00<00:00, 74.1kB/s]\n",
      "Downloading: 100%|██████████| 1.29M/1.29M [00:00<00:00, 9.25MB/s]\n",
      "Downloading: 100%|██████████| 665/665 [00:00<00:00, 1.39MB/s]\n",
      "Downloading: 100%|██████████| 523M/523M [00:17<00:00, 31.4MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(query, top_k=5):\n",
    "    # Embed the query using the text embedder\n",
    "    query_embedding = text_embedder.encode(query, convert_to_tensor=True).cpu().numpy()\n",
    "    \n",
    "    # Search the FAISS index for text\n",
    "    distances, indices = index_text.search(np.array([query_embedding]), top_k)\n",
    "    \n",
    "    # Collect relevant text chunks and limit the number of chunks to avoid exceeding token limit\n",
    "    relevant_chunks = [chunk_metadata[i]['text'] for i in indices[0]]\n",
    "    \n",
    "    # Create context string, ensuring it doesn't exceed the maximum length for the model\n",
    "    context = \"\\n\".join(relevant_chunks)\n",
    "    \n",
    "    # Tokenize the context and query to check the total length\n",
    "    combined_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    inputs = tokenizer.encode(combined_prompt, return_tensors='pt')\n",
    "\n",
    "    # Truncate the input to the model's maximum length\n",
    "    max_length = model.config.n_positions  # Maximum length for the model\n",
    "    if inputs.size(1) > max_length:\n",
    "        inputs = inputs[:, -max_length:]  # Keep only the last max_length tokens\n",
    "\n",
    "    # Generate a response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs, max_length=300, num_return_sequences=1)\n",
    "    \n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 1024, but ``max_length`` is set to 300. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM:\n",
      " campaign increased wages benefits at\n",
      "##le digit wage inflation continue strategically offset expense increases menu price increases operational efficiencies january 2024 increased certain menu prices approximately 15 continue monitor cost pressures competitive landscape well consumer sentiment inform pricing decisions coming quarters believe strength brand consistency operations ongoing execution disciplined development strategy support business model intend continue develop shareholder value selffunded restaurant development ongoing focus operational excellence 2review fourth quarter 2023 financial results revenues fourth quarter ended december 31 2023 1879 million compared 1509 million fourth quarter ended december 25 2022 increase 370 million 245 increase inclusive favorable impact 53rd week resulted incremental revenue 139 million increase revenues primarily attributed opening new restaurants combined increase samerestaurant sales fourth quarter ended december 31 2023 twelve new restaurants opened year ended december 31 2023 one restaurant opened fourth quarter 2022 positively impacted revenues fourth quarter ended december 31 2023 approximately 169 million samerestaurant sales increased 44 fourth quarter ended december 31 2023 attributable increase average check 31 13 increase transactions higher average check driven approximate 60 increase certain menu prices partially offset product mix purpose calculating samerestaurant sales december 31 2023 sales 68 restaurants included comparable restaurant base defined end fiscal 2023 total restaurant operating expenses fourth quarter ended december 31 2023 1421 million compared 1188 million fourth quarter ended december 25 2022 increase 233 million 196 increase restaurant operating expenses driven opening twelve new restaurants year ended december 31 2023 one restaurant opened fourth quarter 2022 additionally food beverage packaging costs negatively impacted 44 increase commodity prices partially offset lower thirdparty delivery commissions labor expense increases also driven incremental investments support team members including annual rate increases higher variablebased compensation operating expenses increased due increase credit card fees cleaning utilities repair maintenance partially offset decrease advertising marketing expenses operating supplies professional fees general administrative expenses fourth quarter ended december 31 2023 216 million compared 177 million fourth quarter ended december 25 2022 increase 38 million 217 increase primarily driven higher variablebased compensation higher advertising expenses increases salaries wages benefits attributable annual rate increases filling open positions partially offset decrease equitybased compensation insurance operating income fourth quarter ended december 31 2023 145 million compared 64 million fourth quarter ended december 25 2022 increase 80 million due aforementioned increase revenues partially offset increase aforementioned expenses net income fourth quarter ended december 31 2023 96 million compared 27 million fourth quarter ended december 25 2022 increase 70 million increase net income primarily due factors driving aforementioned increase operating income decrease interest expense 14 million increase interest income 01 million increase partially offset increase income tax expense 13 million decrease tax receivable agreement liability adjustment 12 million restaurantlevel adjusted ebitda fourth quarter ended december 31 2023 45\n",
      "ended december 31 2023 2309 million compared 2042 million year ended december 25 2022 increase 266 million 130 increase primarily driven opening twelve restaurants year ended december 31 2023 opening three restaurants 2022 55 increase commodity prices partially offset lower thirdparty delivery commissions percentage revenues net food beverage packaging costs decreased 08 year ended december 31 2023 decrease primarily due increase average check lower thirdparty delivery commissions partially offset increase certain commodity prices labor expenses labor expenses include hourly management wages bonuses equitybased compensation payroll taxes workers ’ compensation expense team member benefits factors influence labor costs include wage inflation payroll tax legislation health care costs staffing needs restaurants labor expenses year ended december 31 2023 1739 million compared 1544 million year ended december 25 2022 increase 195 million 126 increase primarily driven opening twelve restaurants year ended december 31 2023 opening three restaurants 2022 incremental investments support team members including annual rate increases higher variablebased compensation percentage revenues net labor decreased 07 year ended december 31 2023 primarily due increase average check partially offset aforementioned incremental hourly rate increases support team members lower transactions higher labor utilization occupancy expenses occupancy expenses primarily consist rent property insurance property taxes occupancy expenses year ended december 31 2023 334 million compared 307 million year ended december 25 2022 increase 27 million 88 primarily driven opening twelve new restaurants year ended december 31 2023 opening three restaurants 2022 percentage revenues occupancy expenses decreased 03 year ended december 31 2023 primarily due increase average check operating expenses operating expenses consist direct marketing expenses utilities expenses incidental operating restaurants credit card fees repairs maintenance operating expenses year ended december 31 2023 766 million compared 653 million year ended december 25 2022 increase 113 million 173 primarily due opening twelve restaurants year ended december 31 2023 opening three restaurants 2022 increase credit card fees utilities repair maintenance expenses insurance partially offset decrease professional fees percentage revenues net operating expenses increased 01 due primarily due aforementioned increases expenses partially offset increase average check portillos inc form 10k 26table contents general administrative expenses general administrative expenses primarily consist costs associated corporate administrative functions support restaurant development operations including marketing advertising costs incurred well legal professional fees general administrative expenses also include equitybased compensation expense general administrative expenses impacted changes team member count costs related strategic growth initiatives general administrative expenses year ended december 31 2023 788 million compared 669 million year ended december 25 2022 increase 119 million 179 increase primarily driven higher variablebased compensation increases salaries wages benefits attributable annual rate increases filling open positions higher advertising software licensing fees partially offset decreases insurance expenses equitybased compensation expense preopening expenses\n",
      "\n",
      "Question: How much did total sales increase in Q4?\n",
      "Answer: Total\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_query = \"How much did total sales increase in Q4?\"\n",
    "response = query_llm(user_query)\n",
    "print(\"Response from LLM:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Results:\n",
      "Filename: PTLO 2023 Q4 Press Release (8K).pdf\n",
      "Chunk Text: 41279 plus general administrative expenses 21550 17707 78835 66892 preopening expenses 3990 2945 9019 4715 depreciation amortization 6525 5104 24313 20907 net income attributable equity method investment 391 276 1401 1083 income loss net 405 129 1035 204 restaurantlevel adjusted ebitda 45736 32049 165171 132506 restaurantlevel adjusted ebitda margin 243 212 243 226 note use 52 53week fiscal year ending sunday prior december 31 fourth quarter 2023 fiscal 2023 consisted 14 weeks 53 weeks respectiv\n",
      "\n",
      "Filename: PTLO 2023 Q4 Press Release (8K).pdf\n",
      "Chunk Text: ##ujuastitnedg e inbteirtedsta ra atensd rr oetshtaeur fraacnttolrse • vt hele imdjpuasctte odf e ubniiotndizaat mioanr agcitniv riteiecso onfc oiulira rteiostnasu arnadn dt wefoinrkiteirosn osn oreui rn ocpleurdaetidoi nns th aen ad pprpoefnitdaibxi tloit tyhi • th pere imsepnatactti oofn recent bank failures marketplace including ability access credit • risks associated reliance certain information technology systems potential failures interruptions • privacy cyber security risks related digit\n",
      "\n",
      "Filename: PTLO 2023 Q4 10K.pdf\n",
      "Chunk Text: year ended december 25 2022 259 year ended december 31 2023 weighted average ownership percentage decreased due secondary offering discussed item 8 financial statements supplementary data key performance indicators nongaap financial measures addition gaap measures presented financial statements use following key performance indicators nongaap financial measures evaluate business measure performance develop financial forecasts make strategic decisions key measures include samerestaurant sales ave\n",
      "\n",
      "Filename: PTLO 2023 Q4 Press Release (8K).pdf\n",
      "Chunk Text: ##i tems consider oure valuation ongoing core operating performance identifiedi n reconciliation neti ncome losst directly comparable gaap measure adjustede bitda adjusted ebitda margin represents adjustede bitdaa percentage revenues net use adjusted ebitda adjusted ebitda margin evaluate ouro perating resultsa nd thee ffectiveness business strategies ii internally benchmarkst compare performance competitors iii factors evaluating management ’ s performance determining incentivec ompensation wea\n",
      "\n",
      "Filename: PTLO 2023 Q4 10K.pdf\n",
      "Chunk Text: ##zation expense 29 million 29 million 64 million years ended december 31 2023 december 25 2022 december 26 2021 respectively included depreciation amortization consolidated statements operations estimated aggregate amortization expense related intangible assets held december 31 2023 next five years thereafter follows thousands estimated amortization 2024 2813 2025 2707 2026 2707 2027 2707 2028 2707 thereafter 15270 28911 portillos inc form 10k 55portillos inc notes consolidated financial statem\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_text(query, index, chunk_metadata, top_k=5):\n",
    "    query_embedding = text_embedder.encode(query, convert_to_tensor=True).cpu().numpy()\n",
    "    _, indices = index.search(np.array([query_embedding]), top_k)\n",
    "    results = [chunk_metadata[i] for i in indices[0]]\n",
    "    return results\n",
    "\n",
    "def search_image(image_path, index, image_metadata, top_k=5):\n",
    "    img_embedding = embed_image(image_path)\n",
    "    _, indices = index.search(np.array([img_embedding]), top_k)\n",
    "    results = [image_metadata[i] for i in indices[0]]\n",
    "    return results\n",
    "\n",
    "# Example text query\n",
    "query = \"financial performance Q3 2023\"\n",
    "text_results = search_text(query, index_text, chunk_metadata)\n",
    "\n",
    "# Example image query (with an image path)\n",
    "# image_path = \"path_to_query_image.png\"\n",
    "# image_results = search_image(image_path, index_image, image_metadata)\n",
    "\n",
    "# Print results\n",
    "print(\"Text Results:\")\n",
    "for result in text_results:\n",
    "    print(f\"Filename: {result['filename']}\\nChunk Text: {result['text'][:500]}\\n\")\n",
    "\n",
    "# print(\"Image Results:\")\n",
    "# for result in image_results:\n",
    "#     print(f\"Filename: {result['filename']}, Image Path: {result['image']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
